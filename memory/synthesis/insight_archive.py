#!/usr/bin/env python3
"""
Insight Archive System for SAM
=============================

Comprehensive archival system for preserving emergent insights generated by SAM's synthesis engine.
Provides storage, retrieval, search, and export capabilities for synthesized knowledge.
"""

import json
import sqlite3
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, asdict
import hashlib
import uuid

from .insight_generator import SynthesizedInsight

logger = logging.getLogger(__name__)

@dataclass
class ArchivedInsight:
    """Represents an archived insight with metadata."""
    archive_id: str
    insight_id: str
    cluster_id: str
    synthesized_text: str
    confidence_score: float
    novelty_score: float
    utility_score: float
    quality_score: float
    source_count: int
    source_documents: List[str]
    synthesis_run_id: str
    archived_at: str
    generated_at: str
    tags: List[str]
    category: str
    summary: str
    metadata: Dict[str, Any]

@dataclass
class InsightCollection:
    """Represents a collection of related insights."""
    collection_id: str
    name: str
    description: str
    insight_ids: List[str]
    created_at: str
    tags: List[str]
    metadata: Dict[str, Any]

class InsightArchive:
    """
    Archive system for storing and managing synthesized insights.
    
    Features:
    - Persistent storage with SQLite database
    - Full-text search capabilities
    - Insight categorization and tagging
    - Export to multiple formats (JSON, Markdown, CSV)
    - Collection management for grouping related insights
    - Quality-based filtering and ranking
    """
    
    def __init__(self, archive_path: str = "data/insight_archive.db"):
        """Initialize the insight archive."""
        self.archive_path = Path(archive_path)
        self.archive_path.parent.mkdir(parents=True, exist_ok=True)
        
        self._init_database()
        logger.info(f"ðŸ“š Insight Archive initialized: {self.archive_path}")
    
    def _init_database(self):
        """Initialize the SQLite database with required tables."""
        with sqlite3.connect(self.archive_path) as conn:
            # Insights table
            conn.execute('''
                CREATE TABLE IF NOT EXISTS insights (
                    archive_id TEXT PRIMARY KEY,
                    insight_id TEXT NOT NULL,
                    cluster_id TEXT NOT NULL,
                    synthesized_text TEXT NOT NULL,
                    confidence_score REAL NOT NULL,
                    novelty_score REAL NOT NULL,
                    utility_score REAL NOT NULL,
                    quality_score REAL NOT NULL,
                    source_count INTEGER NOT NULL,
                    source_documents TEXT NOT NULL,  -- JSON array
                    synthesis_run_id TEXT NOT NULL,
                    archived_at TEXT NOT NULL,
                    generated_at TEXT NOT NULL,
                    tags TEXT NOT NULL,  -- JSON array
                    category TEXT NOT NULL,
                    summary TEXT NOT NULL,
                    metadata TEXT NOT NULL  -- JSON object
                )
            ''')
            
            # Collections table
            conn.execute('''
                CREATE TABLE IF NOT EXISTS collections (
                    collection_id TEXT PRIMARY KEY,
                    name TEXT NOT NULL,
                    description TEXT NOT NULL,
                    insight_ids TEXT NOT NULL,  -- JSON array
                    created_at TEXT NOT NULL,
                    tags TEXT NOT NULL,  -- JSON array
                    metadata TEXT NOT NULL  -- JSON object
                )
            ''')
            
            # Full-text search index
            conn.execute('''
                CREATE VIRTUAL TABLE IF NOT EXISTS insights_fts USING fts5(
                    archive_id,
                    synthesized_text,
                    summary,
                    tags,
                    category,
                    content='insights',
                    content_rowid='rowid'
                )
            ''')
            
            # Create indexes for performance
            conn.execute('CREATE INDEX IF NOT EXISTS idx_quality_score ON insights(quality_score DESC)')
            conn.execute('CREATE INDEX IF NOT EXISTS idx_archived_at ON insights(archived_at DESC)')
            conn.execute('CREATE INDEX IF NOT EXISTS idx_category ON insights(category)')
            conn.execute('CREATE INDEX IF NOT EXISTS idx_synthesis_run ON insights(synthesis_run_id)')
            
            conn.commit()
    
    def archive_insight(self, insight: SynthesizedInsight, 
                       category: str = "general",
                       tags: Optional[List[str]] = None) -> str:
        """Archive a synthesized insight."""
        try:
            # Generate archive ID
            archive_id = str(uuid.uuid4())
            
            # Extract source documents
            source_documents = []
            if hasattr(insight, 'source_chunks') and insight.source_chunks:
                source_documents = list(set(
                    chunk.source for chunk in insight.source_chunks 
                    if hasattr(chunk, 'source')
                ))
            
            # Generate summary (first sentence or first 100 chars)
            summary = self._generate_summary(insight.synthesized_text)
            
            # Calculate quality score
            quality_score = (
                insight.confidence_score * 0.4 + 
                insight.novelty_score * 0.3 + 
                insight.utility_score * 0.3
            )
            
            # Create archived insight
            archived_insight = ArchivedInsight(
                archive_id=archive_id,
                insight_id=insight.insight_id,
                cluster_id=insight.cluster_id,
                synthesized_text=insight.synthesized_text,
                confidence_score=insight.confidence_score,
                novelty_score=insight.novelty_score,
                utility_score=insight.utility_score,
                quality_score=quality_score,
                source_count=len(source_documents),
                source_documents=source_documents,
                synthesis_run_id=getattr(insight, 'synthesis_run_id', 'unknown'),
                archived_at=datetime.now().isoformat(),
                generated_at=insight.generated_at,
                tags=tags or [],
                category=category,
                summary=summary,
                metadata=insight.synthesis_metadata
            )
            
            # Store in database
            with sqlite3.connect(self.archive_path) as conn:
                conn.execute('''
                    INSERT INTO insights VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    archived_insight.archive_id,
                    archived_insight.insight_id,
                    archived_insight.cluster_id,
                    archived_insight.synthesized_text,
                    archived_insight.confidence_score,
                    archived_insight.novelty_score,
                    archived_insight.utility_score,
                    archived_insight.quality_score,
                    archived_insight.source_count,
                    json.dumps(archived_insight.source_documents),
                    archived_insight.synthesis_run_id,
                    archived_insight.archived_at,
                    archived_insight.generated_at,
                    json.dumps(archived_insight.tags),
                    archived_insight.category,
                    archived_insight.summary,
                    json.dumps(archived_insight.metadata)
                ))
                
                # Update FTS index
                conn.execute('''
                    INSERT INTO insights_fts (archive_id, synthesized_text, summary, tags, category)
                    VALUES (?, ?, ?, ?, ?)
                ''', (
                    archive_id,
                    archived_insight.synthesized_text,
                    archived_insight.summary,
                    ' '.join(archived_insight.tags),
                    archived_insight.category
                ))
                
                conn.commit()
            
            logger.info(f"ðŸ“š Archived insight: {archive_id} (quality: {quality_score:.2f})")
            return archive_id
            
        except Exception as e:
            logger.error(f"âŒ Failed to archive insight: {e}")
            raise
    
    def _generate_summary(self, text: str, max_length: int = 150) -> str:
        """Generate a summary of the insight text."""
        # Clean the text
        clean_text = text.strip()
        
        # Try to get first sentence
        sentences = clean_text.split('. ')
        if sentences and len(sentences[0]) <= max_length:
            return sentences[0] + ('.' if not sentences[0].endswith('.') else '')
        
        # Fallback to first max_length characters
        if len(clean_text) <= max_length:
            return clean_text
        
        return clean_text[:max_length].rsplit(' ', 1)[0] + '...'
    
    def get_insights(self, 
                    limit: int = 50,
                    min_quality: float = 0.0,
                    category: Optional[str] = None,
                    tags: Optional[List[str]] = None,
                    since: Optional[datetime] = None) -> List[ArchivedInsight]:
        """Retrieve archived insights with filtering options."""
        try:
            query = '''
                SELECT * FROM insights 
                WHERE quality_score >= ?
            '''
            params = [min_quality]
            
            if category:
                query += ' AND category = ?'
                params.append(category)
            
            if since:
                query += ' AND archived_at >= ?'
                params.append(since.isoformat())
            
            if tags:
                # Simple tag matching - could be enhanced with better logic
                tag_conditions = ' AND '.join(['tags LIKE ?' for _ in tags])
                query += f' AND ({tag_conditions})'
                params.extend([f'%{tag}%' for tag in tags])
            
            query += ' ORDER BY quality_score DESC, archived_at DESC LIMIT ?'
            params.append(limit)
            
            with sqlite3.connect(self.archive_path) as conn:
                cursor = conn.execute(query, params)
                rows = cursor.fetchall()
            
            insights = []
            for row in rows:
                insight = ArchivedInsight(
                    archive_id=row[0],
                    insight_id=row[1],
                    cluster_id=row[2],
                    synthesized_text=row[3],
                    confidence_score=row[4],
                    novelty_score=row[5],
                    utility_score=row[6],
                    quality_score=row[7],
                    source_count=row[8],
                    source_documents=json.loads(row[9]),
                    synthesis_run_id=row[10],
                    archived_at=row[11],
                    generated_at=row[12],
                    tags=json.loads(row[13]),
                    category=row[14],
                    summary=row[15],
                    metadata=json.loads(row[16])
                )
                insights.append(insight)
            
            return insights
            
        except Exception as e:
            logger.error(f"âŒ Failed to retrieve insights: {e}")
            return []

    def search_insights(self, query: str, limit: int = 20) -> List[ArchivedInsight]:
        """Search insights using full-text search."""
        try:
            with sqlite3.connect(self.archive_path) as conn:
                cursor = conn.execute('''
                    SELECT insights.* FROM insights
                    JOIN insights_fts ON insights.archive_id = insights_fts.archive_id
                    WHERE insights_fts MATCH ?
                    ORDER BY rank, quality_score DESC
                    LIMIT ?
                ''', (query, limit))
                rows = cursor.fetchall()

            insights = []
            for row in rows:
                insight = ArchivedInsight(
                    archive_id=row[0],
                    insight_id=row[1],
                    cluster_id=row[2],
                    synthesized_text=row[3],
                    confidence_score=row[4],
                    novelty_score=row[5],
                    utility_score=row[6],
                    quality_score=row[7],
                    source_count=row[8],
                    source_documents=json.loads(row[9]),
                    synthesis_run_id=row[10],
                    archived_at=row[11],
                    generated_at=row[12],
                    tags=json.loads(row[13]),
                    category=row[14],
                    summary=row[15],
                    metadata=json.loads(row[16])
                )
                insights.append(insight)

            return insights

        except Exception as e:
            logger.error(f"âŒ Failed to search insights: {e}")
            return []

    def get_insight_by_id(self, archive_id: str) -> Optional[ArchivedInsight]:
        """Get a specific insight by its archive ID."""
        try:
            with sqlite3.connect(self.archive_path) as conn:
                cursor = conn.execute('''
                    SELECT * FROM insights WHERE archive_id = ?
                ''', (archive_id,))
                row = cursor.fetchone()

            if row:
                return ArchivedInsight(
                    archive_id=row[0],
                    insight_id=row[1],
                    cluster_id=row[2],
                    synthesized_text=row[3],
                    confidence_score=row[4],
                    novelty_score=row[5],
                    utility_score=row[6],
                    quality_score=row[7],
                    source_count=row[8],
                    source_documents=json.loads(row[9]),
                    synthesis_run_id=row[10],
                    archived_at=row[11],
                    generated_at=row[12],
                    tags=json.loads(row[13]),
                    category=row[14],
                    summary=row[15],
                    metadata=json.loads(row[16])
                )
            return None

        except Exception as e:
            logger.error(f"âŒ Failed to get insight by ID: {e}")
            return None

    def get_archive_stats(self) -> Dict[str, Any]:
        """Get statistics about the insight archive."""
        try:
            with sqlite3.connect(self.archive_path) as conn:
                # Basic counts
                total_insights = conn.execute('SELECT COUNT(*) FROM insights').fetchone()[0]

                # Quality distribution
                high_quality = conn.execute(
                    'SELECT COUNT(*) FROM insights WHERE quality_score >= 0.7'
                ).fetchone()[0]

                medium_quality = conn.execute(
                    'SELECT COUNT(*) FROM insights WHERE quality_score >= 0.4 AND quality_score < 0.7'
                ).fetchone()[0]

                # Categories
                categories = conn.execute('''
                    SELECT category, COUNT(*) FROM insights
                    GROUP BY category ORDER BY COUNT(*) DESC
                ''').fetchall()

                # Recent activity (last 7 days)
                week_ago = (datetime.now() - timedelta(days=7)).isoformat()
                recent_insights = conn.execute(
                    'SELECT COUNT(*) FROM insights WHERE archived_at >= ?', (week_ago,)
                ).fetchone()[0]

                # Average quality
                avg_quality = conn.execute(
                    'SELECT AVG(quality_score) FROM insights'
                ).fetchone()[0] or 0.0

                return {
                    'total_insights': total_insights,
                    'high_quality_insights': high_quality,
                    'medium_quality_insights': medium_quality,
                    'low_quality_insights': total_insights - high_quality - medium_quality,
                    'categories': dict(categories),
                    'recent_insights_7d': recent_insights,
                    'average_quality': round(avg_quality, 3),
                    'archive_path': str(self.archive_path)
                }

        except Exception as e:
            logger.error(f"âŒ Failed to get archive stats: {e}")
            return {'error': str(e)}

    def export_insights(self,
                       format: str = "json",
                       output_path: Optional[str] = None,
                       filters: Optional[Dict[str, Any]] = None) -> str:
        """Export insights to various formats."""
        try:
            # Get insights with filters
            insights = self.get_insights(
                limit=filters.get('limit', 1000) if filters else 1000,
                min_quality=filters.get('min_quality', 0.0) if filters else 0.0,
                category=filters.get('category') if filters else None,
                tags=filters.get('tags') if filters else None
            )

            if not output_path:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                output_path = f"exports/insights_export_{timestamp}.{format}"

            output_file = Path(output_path)
            output_file.parent.mkdir(parents=True, exist_ok=True)

            if format.lower() == "json":
                self._export_json(insights, output_file)
            elif format.lower() == "markdown":
                self._export_markdown(insights, output_file)
            elif format.lower() == "csv":
                self._export_csv(insights, output_file)
            else:
                raise ValueError(f"Unsupported export format: {format}")

            logger.info(f"ðŸ“¤ Exported {len(insights)} insights to {output_file}")
            return str(output_file)

        except Exception as e:
            logger.error(f"âŒ Failed to export insights: {e}")
            raise

    def _export_json(self, insights: List[ArchivedInsight], output_file: Path):
        """Export insights to JSON format."""
        export_data = {
            'export_metadata': {
                'exported_at': datetime.now().isoformat(),
                'total_insights': len(insights),
                'format': 'json'
            },
            'insights': [asdict(insight) for insight in insights]
        }

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False)

    def _export_markdown(self, insights: List[ArchivedInsight], output_file: Path):
        """Export insights to Markdown format."""
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("# SAM Emergent Insights Archive\n\n")
            f.write(f"**Exported:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**Total Insights:** {len(insights)}\n\n")

            for i, insight in enumerate(insights, 1):
                f.write(f"## {i}. Insight {insight.archive_id[:8]}\n\n")
                f.write(f"**Category:** {insight.category}\n")
                f.write(f"**Quality Score:** {insight.quality_score:.2f}\n")
                f.write(f"**Generated:** {insight.generated_at[:10]}\n")
                f.write(f"**Sources:** {insight.source_count} documents\n\n")

                if insight.tags:
                    f.write(f"**Tags:** {', '.join(insight.tags)}\n\n")

                f.write("### Insight\n\n")
                f.write(f"{insight.synthesized_text}\n\n")

                if insight.source_documents:
                    f.write("### Source Documents\n\n")
                    for doc in insight.source_documents[:5]:  # Limit to first 5
                        f.write(f"- {doc}\n")
                    f.write("\n")

                f.write("---\n\n")

    def _export_csv(self, insights: List[ArchivedInsight], output_file: Path):
        """Export insights to CSV format."""
        import csv

        with open(output_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)

            # Header
            writer.writerow([
                'Archive ID', 'Category', 'Quality Score', 'Confidence', 'Novelty', 'Utility',
                'Generated At', 'Source Count', 'Tags', 'Summary', 'Full Text'
            ])

            # Data
            for insight in insights:
                writer.writerow([
                    insight.archive_id,
                    insight.category,
                    insight.quality_score,
                    insight.confidence_score,
                    insight.novelty_score,
                    insight.utility_score,
                    insight.generated_at,
                    insight.source_count,
                    ', '.join(insight.tags),
                    insight.summary,
                    insight.synthesized_text.replace('\n', ' ')
                ])


# Global archive instance
_insight_archive = None

def get_insight_archive(archive_path: str = "data/insight_archive.db") -> InsightArchive:
    """Get or create the global insight archive instance."""
    global _insight_archive

    if _insight_archive is None:
        _insight_archive = InsightArchive(archive_path)

    return _insight_archive
