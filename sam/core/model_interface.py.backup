"""
SAM 2.0 Model Interface - Compatibility Layer
=============================================

This module provides a unified interface for SAM to interact with different
model architectures (Transformer vs Hybrid Linear Attention) seamlessly.

The ModelInterface abstraction enables:
- Seamless switching between model architectures
- Fallback mechanisms for reliability
- A/B testing capabilities
- Gradual migration support
- Performance monitoring

Author: SAM Development Team
Version: 1.0.0
"""

import abc
import time
import logging
import requests
from typing import Dict, List, Any, Optional, Union, Tuple
from dataclasses import dataclass
from enum import Enum

logger = logging.getLogger(__name__)

class ModelType(Enum):
    """Supported model types."""
    TRANSFORMER = "transformer"
    HYBRID_LINEAR = "hybrid_linear"
    MOCK = "mock"  # For testing

class ModelStatus(Enum):
    """Model status indicators."""
    READY = "ready"
    LOADING = "loading"
    ERROR = "error"
    UNAVAILABLE = "unavailable"

@dataclass
class ModelConfig:
    """Configuration for model instances."""
    model_type: ModelType
    model_name: str
    api_url: str
    max_context_length: int
    timeout_seconds: int = 120
    temperature: float = 0.7
    max_tokens: int = 1000
    fallback_enabled: bool = True
    performance_monitoring: bool = True

@dataclass
class GenerationRequest:
    """Standardized generation request."""
    prompt: str
    temperature: Optional[float] = None
    max_tokens: Optional[int] = None
    top_p: Optional[float] = None
    stop_sequences: Optional[List[str]] = None
    context_length_hint: Optional[int] = None

@dataclass
class GenerationResponse:
    """Standardized generation response."""
    text: str
    model_type: ModelType
    inference_time: float
    context_length: int
    success: bool
    error_message: Optional[str] = None
    performance_metrics: Optional[Dict[str, Any]] = None

@dataclass
class ModelPerformanceMetrics:
    """Performance metrics for model monitoring."""
    total_requests: int
    successful_requests: int
    average_inference_time: float
    average_context_length: float
    error_rate: float
    last_request_time: float

class ModelInterface(abc.ABC):
    """Abstract base class for all model implementations."""
    
    def __init__(self, config: ModelConfig):
        self.config = config
        self.status = ModelStatus.LOADING
        self.metrics = ModelPerformanceMetrics(
            total_requests=0,
            successful_requests=0,
            average_inference_time=0.0,
            average_context_length=0.0,
            error_rate=0.0,
            last_request_time=0.0
        )
        
    @abc.abstractmethod
    def initialize(self) -> bool:
        """Initialize the model. Returns True if successful."""
        pass
    
    @abc.abstractmethod
    def generate(self, request: GenerationRequest) -> GenerationResponse:
        """Generate text based on the request."""
        pass
    
    @abc.abstractmethod
    def health_check(self) -> bool:
        """Check if the model is healthy and responsive."""
        pass
    
    @abc.abstractmethod
    def get_model_info(self) -> Dict[str, Any]:
        """Get detailed model information."""
        pass
    
    def supports_context_length(self, length: int) -> bool:
        """Check if the model supports the given context length."""
        return length <= self.config.max_context_length
    
    def get_performance_metrics(self) -> ModelPerformanceMetrics:
        """Get current performance metrics."""
        return self.metrics
    
    def _update_metrics(self, response: GenerationResponse, context_length: int):
        """Update performance metrics after a request."""
        if not self.config.performance_monitoring:
            return
            
        self.metrics.total_requests += 1
        self.metrics.last_request_time = time.time()
        
        if response.success:
            self.metrics.successful_requests += 1
            
            # Update running averages
            n = self.metrics.successful_requests
            self.metrics.average_inference_time = (
                (self.metrics.average_inference_time * (n - 1) + response.inference_time) / n
            )
            self.metrics.average_context_length = (
                (self.metrics.average_context_length * (n - 1) + context_length) / n
            )
        
        self.metrics.error_rate = (
            (self.metrics.total_requests - self.metrics.successful_requests) / 
            self.metrics.total_requests
        )

class TransformerModelWrapper(ModelInterface):
    """Wrapper for the current Transformer-based model (Ollama)."""
    
    def __init__(self, config: ModelConfig):
        super().__init__(config)
        self.model_type = ModelType.TRANSFORMER
        
    def initialize(self) -> bool:
        """Initialize the Transformer model."""
        try:
            # Check Ollama availability
            response = requests.get(f"{self.config.api_url}/api/tags", timeout=5)
            if response.status_code == 200:
                self.status = ModelStatus.READY
                logger.info(f"âœ… Transformer model initialized: {self.config.model_name}")
                return True
            else:
                self.status = ModelStatus.ERROR
                logger.error(f"âŒ Ollama not responding: {response.status_code}")
                return False
        except Exception as e:
            self.status = ModelStatus.ERROR
            logger.error(f"âŒ Failed to initialize Transformer model: {e}")
            return False
    
    def generate(self, request: GenerationRequest) -> GenerationResponse:
        """Generate text using Ollama API."""
        start_time = time.time()
        context_length = len(request.prompt.split())
        
        try:
            # Prepare request parameters
            temperature = request.temperature or self.config.temperature
            max_tokens = request.max_tokens or self.config.max_tokens
            
            # Make API request
            response = requests.post(
                f"{self.config.api_url}/api/generate",
                json={
                    "model": self.config.model_name,
                    "prompt": request.prompt,
                    "stream": False,
                    "options": {
                        "temperature": temperature,
                        "top_p": request.top_p or 0.9,
                        "max_tokens": max_tokens
                    }
                },
                timeout=self.config.timeout_seconds
            )
            
            inference_time = time.time() - start_time
            
            if response.status_code == 200:
                result = response.json()
                generated_text = result.get('response', '')
                
                gen_response = GenerationResponse(
                    text=generated_text,
                    model_type=self.model_type,
                    inference_time=inference_time,
                    context_length=context_length,
                    success=True,
                    performance_metrics={
                        'api_response_time': inference_time,
                        'context_tokens': context_length
                    }
                )
                
                self._update_metrics(gen_response, context_length)
                return gen_response
            else:
                error_msg = f"API error: {response.status_code}"
                gen_response = GenerationResponse(
                    text="",
                    model_type=self.model_type,
                    inference_time=inference_time,
                    context_length=context_length,
                    success=False,
                    error_message=error_msg
                )
                
                self._update_metrics(gen_response, context_length)
                return gen_response
                
        except Exception as e:
            inference_time = time.time() - start_time
            error_msg = f"Generation failed: {str(e)}"
            
            gen_response = GenerationResponse(
                text="",
                model_type=self.model_type,
                inference_time=inference_time,
                context_length=context_length,
                success=False,
                error_message=error_msg
            )
            
            self._update_metrics(gen_response, context_length)
            return gen_response
    
    def health_check(self) -> bool:
        """Check Transformer model health."""
        try:
            response = requests.get(f"{self.config.api_url}/api/tags", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get Transformer model information."""
        try:
            response = requests.post(
                f"{self.config.api_url}/api/show",
                json={"name": self.config.model_name},
                timeout=10
            )
            if response.status_code == 200:
                return response.json()
            else:
                return {"error": f"Failed to get model info: {response.status_code}"}
        except Exception as e:
            return {"error": f"Failed to get model info: {str(e)}"}

class HybridModelWrapper(ModelInterface):
    """Wrapper for the new Hybrid Linear Attention model."""
    
    def __init__(self, config: ModelConfig):
        super().__init__(config)
        self.model_type = ModelType.HYBRID_LINEAR
        self.model = None  # Will hold the actual hybrid model
        
    def initialize(self) -> bool:
        """Initialize the Hybrid model."""
        try:
            logger.info("ðŸ”„ Initializing HGRN-2 Hybrid model...")

            # Import hybrid model components
            try:
                from sam.models.sam_hybrid_model import SAMHybridModel
                from sam.models.hybrid_config import HybridModelConfig

                # Create hybrid model configuration
                hybrid_config = HybridModelConfig(
                    vocab_size=32000,  # Standard vocabulary size
                    hidden_size=4096,  # 8B model equivalent
                    num_layers=32,     # Standard depth
                    linear_ratio=3,    # 3:1 linear to full attention ratio
                    max_context_length=self.config.max_context_length,
                    model_type="hgrn2"
                )

                # Initialize the hybrid model (untrained)
                self.model = SAMHybridModel(hybrid_config)
                logger.info(f"âœ… HGRN-2 model structure created with {self.model.get_parameter_count():,} parameters")

                # Set model to evaluation mode
                self.model.eval()

                self.status = ModelStatus.READY
                logger.info(f"âœ… Hybrid model initialized: {self.config.model_name}")
                return True

            except ImportError as e:
                logger.warning(f"Hybrid model components not yet implemented: {e}")
                logger.info("ðŸ”„ Using placeholder implementation for Phase 1 development")

                # Placeholder for development
                self.model = "placeholder_hgrn2_model"
                self.status = ModelStatus.READY
                return True

        except Exception as e:
            self.status = ModelStatus.ERROR
            logger.error(f"âŒ Failed to initialize Hybrid model: {e}")
            return False
    
    def generate(self, request: GenerationRequest) -> GenerationResponse:
        """Generate text using Hybrid model."""
        start_time = time.time()
        context_length = len(request.prompt.split())
        
        try:
            # TODO: Implement actual hybrid model generation
            # For now, return a placeholder response
            
            # Simulate processing time based on context length
            # Hybrid models should scale better than transformers
            simulated_time = 0.01 + (context_length * 0.00001)  # Linear scaling
            time.sleep(simulated_time)
            
            generated_text = f"[Hybrid Model Response] Processed {context_length} tokens efficiently."
            inference_time = time.time() - start_time
            
            gen_response = GenerationResponse(
                text=generated_text,
                model_type=self.model_type,
                inference_time=inference_time,
                context_length=context_length,
                success=True,
                performance_metrics={
                    'hybrid_processing_time': inference_time,
                    'context_tokens': context_length,
                    'linear_attention_ratio': 0.75  # 3:1 ratio
                }
            )
            
            self._update_metrics(gen_response, context_length)
            return gen_response
            
        except Exception as e:
            inference_time = time.time() - start_time
            error_msg = f"Hybrid generation failed: {str(e)}"
            
            gen_response = GenerationResponse(
                text="",
                model_type=self.model_type,
                inference_time=inference_time,
                context_length=context_length,
                success=False,
                error_message=error_msg
            )
            
            self._update_metrics(gen_response, context_length)
            return gen_response
    
    def health_check(self) -> bool:
        """Check Hybrid model health."""
        # TODO: Implement actual health check
        return self.status == ModelStatus.READY
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get Hybrid model information."""
        return {
            "model_type": "hybrid_linear_attention",
            "architecture": "HGRN-2",
            "ratio": "3:1 (linear:full)",
            "max_context_length": self.config.max_context_length,
            "status": self.status.value,
            "implementation": "placeholder"  # Will be updated in Phase 1
        }

class ModelManager:
    """Manages multiple model instances with fallback support."""
    
    def __init__(self):
        self.models: Dict[str, ModelInterface] = {}
        self.primary_model: Optional[str] = None
        self.fallback_model: Optional[str] = None
        
    def register_model(self, name: str, model: ModelInterface) -> bool:
        """Register a model instance."""
        if model.initialize():
            self.models[name] = model
            logger.info(f"âœ… Model registered: {name}")
            return True
        else:
            logger.error(f"âŒ Failed to register model: {name}")
            return False
    
    def set_primary_model(self, name: str):
        """Set the primary model for generation."""
        if name in self.models:
            self.primary_model = name
            logger.info(f"ðŸŽ¯ Primary model set to: {name}")
        else:
            raise ValueError(f"Model {name} not registered")
    
    def set_fallback_model(self, name: str):
        """Set the fallback model."""
        if name in self.models:
            self.fallback_model = name
            logger.info(f"ðŸ›¡ï¸ Fallback model set to: {name}")
        else:
            raise ValueError(f"Model {name} not registered")
    
    def generate(self, request: GenerationRequest) -> GenerationResponse:
        """Generate text with automatic fallback support."""
        # Try primary model first
        if self.primary_model and self.primary_model in self.models:
            primary = self.models[self.primary_model]
            
            if primary.health_check():
                response = primary.generate(request)
                if response.success:
                    return response
                else:
                    logger.warning(f"Primary model failed: {response.error_message}")
        
        # Fallback to secondary model
        if self.fallback_model and self.fallback_model in self.models:
            fallback = self.models[self.fallback_model]
            
            if fallback.health_check():
                logger.info("ðŸ›¡ï¸ Using fallback model")
                response = fallback.generate(request)
                return response
        
        # No working models available
        return GenerationResponse(
            text="",
            model_type=ModelType.MOCK,
            inference_time=0.0,
            context_length=0,
            success=False,
            error_message="No working models available"
        )
    
    def get_model_status(self) -> Dict[str, Dict[str, Any]]:
        """Get status of all registered models."""
        status = {}
        for name, model in self.models.items():
            status[name] = {
                "type": model.model_type.value,
                "status": model.status.value,
                "health": model.health_check(),
                "metrics": model.get_performance_metrics(),
                "config": {
                    "max_context_length": model.config.max_context_length,
                    "timeout_seconds": model.config.timeout_seconds
                }
            }
        return status

# Global model manager instance
_model_manager = None

def get_model_manager() -> ModelManager:
    """Get the global model manager instance."""
    global _model_manager
    if _model_manager is None:
        _model_manager = ModelManager()
    return _model_manager

def initialize_sam_models() -> ModelManager:
    """Initialize SAM's model configuration."""
    manager = get_model_manager()

    # Configure Transformer model (current)
    transformer_config = ModelConfig(
        model_type=ModelType.TRANSFORMER,
        model_name="hf.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF:Q4_K_M",
        api_url="http://localhost:11434",
        max_context_length=16000,  # Based on baseline testing
        timeout_seconds=120,
        fallback_enabled=True
    )

    # Configure Hybrid model (future)
    hybrid_config = ModelConfig(
        model_type=ModelType.HYBRID_LINEAR,
        model_name="sam-hybrid-hgrn2-3to1",
        api_url="http://localhost:11435",  # Different port for hybrid model
        max_context_length=100000,  # Target for hybrid model
        timeout_seconds=60,  # Should be faster
        fallback_enabled=True
    )

    # Register models
    transformer_model = TransformerModelWrapper(transformer_config)
    hybrid_model = HybridModelWrapper(hybrid_config)

    manager.register_model("transformer", transformer_model)
    manager.register_model("hybrid", hybrid_model)

    # Set primary and fallback
    manager.set_primary_model("transformer")  # Current default
    manager.set_fallback_model("transformer")  # Self-fallback for now

    return manager

# Utility functions for SAM integration
def switch_to_hybrid_model():
    """Switch SAM to use the hybrid model as primary."""
    manager = get_model_manager()
    manager.set_primary_model("hybrid")
    manager.set_fallback_model("transformer")
    logger.info("ðŸ”„ Switched to hybrid model with transformer fallback")

def switch_to_transformer_model():
    """Switch SAM to use the transformer model as primary."""
    manager = get_model_manager()
    manager.set_primary_model("transformer")
    logger.info("ðŸ”„ Switched to transformer model")

def get_current_model_info() -> Dict[str, Any]:
    """Get information about the currently active model."""
    manager = get_model_manager()
    status = manager.get_model_status()

    return {
        "primary_model": manager.primary_model,
        "fallback_model": manager.fallback_model,
        "model_status": status,
        "total_models": len(manager.models)
    }
