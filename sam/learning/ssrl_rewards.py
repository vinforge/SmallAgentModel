#!/usr/bin/env python3
"""
SSRL Reward Functions
=====================

Reward functions for Self-Supervised Reasoning and Learning (SSRL) fine-tuning.
Implements both format rewards (structure validation) and outcome rewards
(answer quality assessment) for training the SSRL-LoRA adapter.

This module provides:
- SSRLFormatReward: Validates SSRL structure and content quality
- SSRLOutcomeReward: Evaluates answer accuracy and semantic similarity
- SSRLCombinedReward: Combines format and outcome rewards

Author: SAM Development Team
Version: 1.0.0
"""

import re
import string
import logging
from typing import Dict, Any, List, Tuple, Optional, Union, Set
from dataclasses import dataclass, field
from enum import Enum

# Configure module logger
logger = logging.getLogger(__name__)

# Constants
DEFAULT_REQUIRED_TAGS = ['think', 'search', 'information', 'confidence', 'answer']
MIN_CONTENT_LENGTH = 10
DEFAULT_CONFIDENCE_THRESHOLD = 0.7
EXACT_MATCH_WEIGHT = 0.4
SEMANTIC_WEIGHT = 0.6
FORMAT_WEIGHT = 0.3
OUTCOME_WEIGHT = 0.7


class SSRLRewardType(Enum):
    """Types of SSRL rewards."""
    FORMAT = "format"
    OUTCOME = "outcome"
    COMBINED = "combined"


@dataclass
class SSRLRewardResult:
    """
    Result from SSRL reward calculation.

    Attributes:
        reward_type: Type of reward calculation performed
        score: Reward score between 0.0 and 1.0
        details: Additional details about the calculation
        explanation: Human-readable explanation of the score
        success: Whether the calculation was successful
        error: Error message if calculation failed
        metadata: Additional metadata for debugging
    """
    reward_type: SSRLRewardType
    score: float  # 0.0 to 1.0
    details: Dict[str, Any]
    explanation: str
    success: bool
    error: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

    def __post_init__(self) -> None:
        """Validate reward result after initialization."""
        if not 0.0 <= self.score <= 1.0:
            logger.warning(f"Score {self.score} is outside valid range [0.0, 1.0]")
            self.score = max(0.0, min(1.0, self.score))

        if self.success and self.error:
            logger.warning("Success=True but error is set, clearing error")
            self.error = None


class SSRLFormatReward:
    """
    Format reward function for SSRL structured output validation.
    
    Validates that the model generates properly structured SSRL responses
    with required tags and appropriate content.
    """
    
    def __init__(self,
                 required_tags: Optional[List[str]] = None,
                 min_content_length: int = MIN_CONTENT_LENGTH,
                 strict_mode: bool = False) -> None:
        """
        Initialize format reward function.

        Args:
            required_tags: List of required SSRL tags. Defaults to standard SSRL tags.
            min_content_length: Minimum content length per tag. Must be positive.
            strict_mode: Whether to enforce strict tag ordering.

        Raises:
            ValueError: If min_content_length is not positive or required_tags is empty.
        """
        if min_content_length <= 0:
            raise ValueError(f"min_content_length must be positive, got {min_content_length}")

        self.required_tags = required_tags or DEFAULT_REQUIRED_TAGS.copy()

        if not self.required_tags:
            raise ValueError("required_tags cannot be empty")

        self.min_content_length = min_content_length
        self.strict_mode = strict_mode

        # Validate tags
        for tag in self.required_tags:
            if not isinstance(tag, str) or not tag.strip():
                raise ValueError(f"Invalid tag: {tag!r}")

        logger.info(
            f"SSRLFormatReward initialized: {len(self.required_tags)} tags, "
            f"min_length={min_content_length}, strict={strict_mode}"
        )
    
    def calculate_reward(self, generated_text: str) -> SSRLRewardResult:
        """
        Calculate format reward for generated SSRL text.

        Args:
            generated_text: The full text generated by the model

        Returns:
            SSRLRewardResult with format score and details

        Raises:
            TypeError: If generated_text is not a string
        """
        if not isinstance(generated_text, str):
            raise TypeError(f"generated_text must be str, got {type(generated_text)}")

        if not generated_text.strip():
            return SSRLRewardResult(
                reward_type=SSRLRewardType.FORMAT,
                score=0.0,
                details={'error': 'Empty or whitespace-only text'},
                explanation="No content to evaluate",
                success=False,
                error="Empty input text"
            )

        try:
            # Parse the structured response
            parsed_sections = self._parse_ssrl_structure(generated_text)

            # Calculate individual scores
            structure_score = self._calculate_structure_score(parsed_sections)
            completeness_score = self._calculate_completeness_score(parsed_sections)
            quality_score = self._calculate_content_quality_score(parsed_sections)

            # Combine scores with validation
            scores = [structure_score, completeness_score, quality_score]
            if not all(0.0 <= score <= 1.0 for score in scores):
                logger.warning(f"Invalid scores detected: {scores}")
                scores = [max(0.0, min(1.0, score)) for score in scores]

            final_score = sum(scores) / len(scores)

            # Create detailed explanation
            explanation = self._create_format_explanation(
                structure_score, completeness_score, quality_score, parsed_sections
            )

            details = {
                'structure_score': structure_score,
                'completeness_score': completeness_score,
                'quality_score': quality_score,
                'parsed_sections': {k: len(v) for k, v in parsed_sections.items()},
                'required_tags_found': len([k for k, v in parsed_sections.items() if v.strip()]),
                'required_tags_total': len(self.required_tags),
                'text_length': len(generated_text),
                'strict_mode': self.strict_mode
            }

            return SSRLRewardResult(
                reward_type=SSRLRewardType.FORMAT,
                score=final_score,
                details=details,
                explanation=explanation,
                success=True,
                metadata={'processing_time': 0.0}  # Could add timing if needed
            )

        except Exception as e:
            logger.error(f"Format reward calculation failed: {e}", exc_info=True)
            return SSRLRewardResult(
                reward_type=SSRLRewardType.FORMAT,
                score=0.0,
                details={'error': str(e), 'error_type': type(e).__name__},
                explanation=f"Format reward calculation failed: {e}",
                success=False,
                error=str(e)
            )
    
    def _parse_ssrl_structure(self, text: str) -> Dict[str, str]:
        """
        Parse SSRL structured response into sections.
        
        Args:
            text: Generated text with SSRL tags
            
        Returns:
            Dictionary mapping tag names to content
        """
        sections = {}
        
        # Define patterns for each required tag
        for tag in self.required_tags:
            pattern = rf'<{tag}>(.*?)</{tag}>'
            match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
            
            if match:
                content = match.group(1).strip()
                sections[tag] = content
            else:
                sections[tag] = ""
        
        return sections
    
    def _calculate_structure_score(self, parsed_sections: Dict[str, str]) -> float:
        """Calculate score based on structural completeness."""
        # Check if all required tags are present with content
        tags_with_content = sum(1 for tag in self.required_tags 
                               if parsed_sections.get(tag, "").strip())
        
        structure_score = tags_with_content / len(self.required_tags)
        
        # Bonus for proper tag ordering (if strict mode)
        if self.strict_mode:
            # Check if tags appear in the expected order
            # This is a simplified check - could be enhanced
            order_bonus = 0.1 if self._check_tag_order(parsed_sections) else 0.0
            structure_score = min(1.0, structure_score + order_bonus)
        
        return structure_score
    
    def _calculate_completeness_score(self, parsed_sections: Dict[str, str]) -> float:
        """Calculate score based on content completeness."""
        completeness_scores = []
        
        for tag in self.required_tags:
            content = parsed_sections.get(tag, "")
            
            if not content.strip():
                completeness_scores.append(0.0)
            elif len(content) < self.min_content_length:
                # Partial credit for short content
                completeness_scores.append(len(content) / self.min_content_length * 0.5)
            else:
                completeness_scores.append(1.0)
        
        return sum(completeness_scores) / len(completeness_scores)
    
    def _calculate_content_quality_score(self, parsed_sections: Dict[str, str]) -> float:
        """Calculate score based on content quality indicators."""
        quality_factors = []
        
        # Check thinking section quality
        thinking = parsed_sections.get('think', '')
        if thinking:
            # Look for reasoning indicators
            reasoning_words = ['because', 'therefore', 'since', 'however', 'although']
            reasoning_count = sum(1 for word in reasoning_words if word in thinking.lower())
            thinking_quality = min(1.0, reasoning_count / 3.0)  # Normalize to max 3 words
            quality_factors.append(thinking_quality)
        
        # Check search section quality
        search = parsed_sections.get('search', '')
        if search:
            # Look for knowledge indicators
            knowledge_words = ['know', 'information', 'fact', 'concept', 'example']
            knowledge_count = sum(1 for word in knowledge_words if word in search.lower())
            search_quality = min(1.0, knowledge_count / 2.0)  # Normalize to max 2 words
            quality_factors.append(search_quality)
        
        # Check confidence section
        confidence = parsed_sections.get('confidence', '')
        if confidence:
            # Check if confidence is a valid number
            try:
                conf_value = float(confidence.strip())
                if 0.0 <= conf_value <= 1.0:
                    quality_factors.append(1.0)
                else:
                    quality_factors.append(0.5)  # Valid number but out of range
            except ValueError:
                quality_factors.append(0.0)  # Invalid confidence format
        
        # Check answer section quality
        answer = parsed_sections.get('answer', '')
        if answer:
            # Basic quality check - length and completeness
            if len(answer) >= 20:  # Minimum meaningful answer length
                quality_factors.append(1.0)
            else:
                quality_factors.append(len(answer) / 20.0)
        
        return sum(quality_factors) / len(quality_factors) if quality_factors else 0.0
    
    def _check_tag_order(self, parsed_sections: Dict[str, str]) -> bool:
        """Check if tags appear in the expected order (simplified)."""
        # This is a basic implementation - could be enhanced
        # For now, just check that we have the main tags
        required_order = ['think', 'search', 'information', 'confidence', 'answer']
        found_tags = [tag for tag in required_order if parsed_sections.get(tag, "").strip()]
        
        # If we have at least 3 tags in order, consider it good
        return len(found_tags) >= 3
    
    def _create_format_explanation(self, 
                                 structure_score: float,
                                 completeness_score: float, 
                                 quality_score: float,
                                 parsed_sections: Dict[str, str]) -> str:
        """Create human-readable explanation of format score."""
        explanation_parts = []
        
        # Structure explanation
        tags_found = sum(1 for tag in self.required_tags if parsed_sections.get(tag, "").strip())
        explanation_parts.append(f"Structure: {tags_found}/{len(self.required_tags)} required tags found ({structure_score:.2f})")
        
        # Completeness explanation
        explanation_parts.append(f"Completeness: Content quality across tags ({completeness_score:.2f})")
        
        # Quality explanation
        explanation_parts.append(f"Quality: Content depth and reasoning indicators ({quality_score:.2f})")
        
        return "; ".join(explanation_parts)


class SSRLOutcomeReward:
    """
    Outcome reward function for SSRL answer quality assessment.
    
    Evaluates the quality of the final answer compared to ground truth
    using exact match and semantic similarity.
    """
    
    def __init__(self, 
                 exact_match_weight: float = 0.4,
                 semantic_weight: float = 0.6,
                 use_llm_judge: bool = True):
        """
        Initialize outcome reward function.
        
        Args:
            exact_match_weight: Weight for exact match scoring
            semantic_weight: Weight for semantic similarity scoring
            use_llm_judge: Whether to use LLM-as-a-Judge for evaluation
        """
        self.exact_match_weight = exact_match_weight
        self.semantic_weight = semantic_weight
        self.use_llm_judge = use_llm_judge
        
        logger.info(f"SSRLOutcomeReward initialized (EM weight: {exact_match_weight}, Semantic weight: {semantic_weight})")
    
    def calculate_reward(self, 
                        generated_answer: str, 
                        ground_truth: str,
                        question: str = None) -> SSRLRewardResult:
        """
        Calculate outcome reward for generated answer.
        
        Args:
            generated_answer: The extracted <answer> text from SSRL output
            ground_truth: The correct answer from training data
            question: Original question (optional, for context)
            
        Returns:
            SSRLRewardResult with outcome score and details
        """
        try:
            # Calculate exact match score
            exact_match_score = self._calculate_exact_match(generated_answer, ground_truth)
            
            # Calculate semantic similarity score
            semantic_score = self._calculate_semantic_similarity(
                generated_answer, ground_truth, question
            )
            
            # Combine scores
            final_score = (self.exact_match_weight * exact_match_score + 
                          self.semantic_weight * semantic_score)
            
            # Create explanation
            explanation = self._create_outcome_explanation(
                exact_match_score, semantic_score, generated_answer, ground_truth
            )
            
            details = {
                'exact_match_score': exact_match_score,
                'semantic_score': semantic_score,
                'exact_match_weight': self.exact_match_weight,
                'semantic_weight': self.semantic_weight,
                'generated_length': len(generated_answer),
                'ground_truth_length': len(ground_truth)
            }
            
            return SSRLRewardResult(
                reward_type=SSRLRewardType.OUTCOME,
                score=final_score,
                details=details,
                explanation=explanation,
                success=True
            )
            
        except Exception as e:
            logger.error(f"Outcome reward calculation failed: {e}")
            return SSRLRewardResult(
                reward_type=SSRLRewardType.OUTCOME,
                score=0.0,
                details={'error': str(e)},
                explanation=f"Outcome reward calculation failed: {e}",
                success=False
            )

    def _calculate_exact_match(self, generated: str, ground_truth: str) -> float:
        """Calculate exact match score between generated and ground truth."""
        # Normalize both strings for comparison
        gen_normalized = self._normalize_text(generated)
        gt_normalized = self._normalize_text(ground_truth)

        # Exact match
        if gen_normalized == gt_normalized:
            return 1.0

        # Partial credit for substring matches
        if gen_normalized in gt_normalized or gt_normalized in gen_normalized:
            return 0.7

        # Check for key phrase matches
        gen_words = set(gen_normalized.split())
        gt_words = set(gt_normalized.split())

        if len(gt_words) > 0:
            overlap = len(gen_words.intersection(gt_words))
            return min(0.5, overlap / len(gt_words))

        return 0.0

    def _calculate_semantic_similarity(self,
                                     generated: str,
                                     ground_truth: str,
                                     question: str = None) -> float:
        """Calculate semantic similarity using LLM-as-a-Judge."""
        if not self.use_llm_judge:
            # Fallback to simple word overlap
            return self._simple_semantic_similarity(generated, ground_truth)

        try:
            # Use LLM-as-a-Judge for semantic evaluation
            judge_prompt = self._create_judge_prompt(generated, ground_truth, question)

            # Get LLM evaluation
            from sam.models.model_interface import get_model_interface
            model_interface = get_model_interface()

            judge_response = model_interface.generate_response(
                prompt=judge_prompt,
                max_tokens=200,
                temperature=0.1,  # Low temperature for consistent evaluation
                system_message="You are an expert evaluator assessing answer quality."
            )

            # Parse the judge's score
            score = self._parse_judge_score(judge_response)
            return score

        except Exception as e:
            logger.warning(f"LLM-as-a-Judge failed, using fallback: {e}")
            return self._simple_semantic_similarity(generated, ground_truth)

    def _create_judge_prompt(self, generated: str, ground_truth: str, question: str = None) -> str:
        """Create prompt for LLM-as-a-Judge evaluation."""
        base_prompt = f"""You are evaluating the quality of an AI-generated answer compared to the correct answer.

Generated Answer: "{generated}"

Correct Answer: "{ground_truth}"
"""

        if question:
            base_prompt = f"""You are evaluating the quality of an AI-generated answer compared to the correct answer.

Question: "{question}"

Generated Answer: "{generated}"

Correct Answer: "{ground_truth}"
"""

        base_prompt += """
Please evaluate the generated answer on a scale of 0.0 to 1.0 based on:
- Factual accuracy compared to the correct answer
- Completeness of the response
- Clarity and coherence

Provide your score as a single number between 0.0 and 1.0.
Score: """

        return base_prompt

    def _parse_judge_score(self, judge_response: str) -> float:
        """Parse the numerical score from judge response."""
        # Look for a number between 0.0 and 1.0
        import re

        # Try to find a decimal number
        matches = re.findall(r'\b0?\.\d+\b|\b1\.0\b|\b[01]\b', judge_response)

        if matches:
            try:
                score = float(matches[0])
                return max(0.0, min(1.0, score))  # Clamp to valid range
            except ValueError:
                pass

        # Fallback: look for percentage and convert
        percent_matches = re.findall(r'(\d+)%', judge_response)
        if percent_matches:
            try:
                percent = float(percent_matches[0])
                return max(0.0, min(1.0, percent / 100.0))
            except ValueError:
                pass

        # Default to middle score if parsing fails
        logger.warning(f"Could not parse judge score from: {judge_response}")
        return 0.5

    def _simple_semantic_similarity(self, generated: str, ground_truth: str) -> float:
        """Simple semantic similarity based on word overlap."""
        gen_words = set(self._normalize_text(generated).split())
        gt_words = set(self._normalize_text(ground_truth).split())

        if len(gt_words) == 0:
            return 0.0

        # Jaccard similarity
        intersection = len(gen_words.intersection(gt_words))
        union = len(gen_words.union(gt_words))

        if union == 0:
            return 0.0

        return intersection / union

    def _normalize_text(self, text: str) -> str:
        """
        Normalize text for comparison.

        Args:
            text: Text to normalize

        Returns:
            Normalized text with lowercase, no punctuation, and normalized whitespace

        Raises:
            TypeError: If text is not a string
        """
        if not isinstance(text, str):
            raise TypeError(f"Expected str, got {type(text)}")

        if not text.strip():
            return ""

        try:
            # Convert to lowercase
            normalized = text.lower()

            # Remove punctuation
            normalized = normalized.translate(str.maketrans('', '', string.punctuation))

            # Remove extra whitespace and normalize
            normalized = ' '.join(normalized.split())

            return normalized

        except Exception as e:
            logger.warning(f"Text normalization failed for text length {len(text)}: {e}")
            # Fallback: just lowercase and strip
            return text.lower().strip()

    def _create_outcome_explanation(self,
                                  exact_match_score: float,
                                  semantic_score: float,
                                  generated: str,
                                  ground_truth: str) -> str:
        """Create human-readable explanation of outcome score."""
        explanation_parts = []

        # Exact match explanation
        if exact_match_score == 1.0:
            explanation_parts.append("Perfect exact match")
        elif exact_match_score > 0.5:
            explanation_parts.append(f"Good text overlap ({exact_match_score:.2f})")
        else:
            explanation_parts.append(f"Limited text overlap ({exact_match_score:.2f})")

        # Semantic explanation
        if semantic_score > 0.8:
            explanation_parts.append("High semantic similarity")
        elif semantic_score > 0.6:
            explanation_parts.append("Moderate semantic similarity")
        else:
            explanation_parts.append("Low semantic similarity")

        return "; ".join(explanation_parts)


class SSRLCombinedReward:
    """
    Combined reward function that merges format and outcome rewards.
    """

    def __init__(self,
                 format_weight: float = 0.3,
                 outcome_weight: float = 0.7):
        """
        Initialize combined reward function.

        Args:
            format_weight: Weight for format reward
            outcome_weight: Weight for outcome reward
        """
        self.format_weight = format_weight
        self.outcome_weight = outcome_weight

        self.format_reward = SSRLFormatReward()
        self.outcome_reward = SSRLOutcomeReward()

        logger.info(f"SSRLCombinedReward initialized (Format: {format_weight}, Outcome: {outcome_weight})")

    def calculate_reward(self,
                        generated_text: str,
                        ground_truth: str,
                        question: str = None) -> SSRLRewardResult:
        """
        Calculate combined reward from format and outcome.

        Args:
            generated_text: Full SSRL-structured text
            ground_truth: Correct answer
            question: Original question

        Returns:
            SSRLRewardResult with combined score
        """
        try:
            # Calculate format reward
            format_result = self.format_reward.calculate_reward(generated_text)

            # Extract answer from generated text for outcome evaluation
            generated_answer = self._extract_answer(generated_text)

            # Calculate outcome reward
            outcome_result = self.outcome_reward.calculate_reward(
                generated_answer, ground_truth, question
            )

            # Combine scores
            if format_result.success and outcome_result.success:
                combined_score = (self.format_weight * format_result.score +
                                self.outcome_weight * outcome_result.score)
            elif format_result.success:
                combined_score = format_result.score * 0.5  # Penalty for outcome failure
            elif outcome_result.success:
                combined_score = outcome_result.score * 0.5  # Penalty for format failure
            else:
                combined_score = 0.0

            # Create combined explanation
            explanation = f"Format ({self.format_weight}): {format_result.explanation}; Outcome ({self.outcome_weight}): {outcome_result.explanation}"

            # Combine details
            details = {
                'format_score': format_result.score,
                'outcome_score': outcome_result.score,
                'format_weight': self.format_weight,
                'outcome_weight': self.outcome_weight,
                'format_details': format_result.details,
                'outcome_details': outcome_result.details,
                'extracted_answer': generated_answer
            }

            return SSRLRewardResult(
                reward_type=SSRLRewardType.COMBINED,
                score=combined_score,
                details=details,
                explanation=explanation,
                success=format_result.success or outcome_result.success
            )

        except Exception as e:
            logger.error(f"Combined reward calculation failed: {e}")
            return SSRLRewardResult(
                reward_type=SSRLRewardType.COMBINED,
                score=0.0,
                details={'error': str(e)},
                explanation=f"Combined reward calculation failed: {e}",
                success=False
            )

    def _extract_answer(self, generated_text: str) -> str:
        """Extract the answer section from SSRL-structured text."""
        import re

        # Look for <answer> tags
        match = re.search(r'<answer>(.*?)</answer>', generated_text, re.DOTALL | re.IGNORECASE)

        if match:
            return match.group(1).strip()

        # Fallback: return the last substantial paragraph
        lines = [line.strip() for line in generated_text.split('\n') if line.strip()]
        if lines:
            return lines[-1]

        return generated_text.strip()


# Convenience functions for easy access
def get_format_reward(
    required_tags: Optional[List[str]] = None,
    min_content_length: int = MIN_CONTENT_LENGTH,
    strict_mode: bool = False
) -> SSRLFormatReward:
    """
    Get a format reward instance with optional configuration.

    Args:
        required_tags: List of required SSRL tags
        min_content_length: Minimum content length per tag
        strict_mode: Whether to enforce strict tag ordering

    Returns:
        Configured SSRLFormatReward instance
    """
    return SSRLFormatReward(
        required_tags=required_tags,
        min_content_length=min_content_length,
        strict_mode=strict_mode
    )


def get_outcome_reward(
    exact_match_weight: float = EXACT_MATCH_WEIGHT,
    semantic_weight: float = SEMANTIC_WEIGHT,
    use_llm_judge: bool = True
) -> SSRLOutcomeReward:
    """
    Get an outcome reward instance with optional configuration.

    Args:
        exact_match_weight: Weight for exact match scoring
        semantic_weight: Weight for semantic similarity scoring
        use_llm_judge: Whether to use LLM-as-a-Judge for evaluation

    Returns:
        Configured SSRLOutcomeReward instance
    """
    return SSRLOutcomeReward(
        exact_match_weight=exact_match_weight,
        semantic_weight=semantic_weight,
        use_llm_judge=use_llm_judge
    )


def get_combined_reward(
    format_weight: float = FORMAT_WEIGHT,
    outcome_weight: float = OUTCOME_WEIGHT
) -> SSRLCombinedReward:
    """
    Get a combined reward instance with optional configuration.

    Args:
        format_weight: Weight for format reward
        outcome_weight: Weight for outcome reward

    Returns:
        Configured SSRLCombinedReward instance

    Raises:
        ValueError: If weights don't sum to approximately 1.0
    """
    if abs(format_weight + outcome_weight - 1.0) > 0.01:
        raise ValueError(
            f"Weights should sum to 1.0, got {format_weight + outcome_weight:.3f}"
        )

    return SSRLCombinedReward(
        format_weight=format_weight,
        outcome_weight=outcome_weight
    )


def validate_ssrl_response(text: str) -> Dict[str, Any]:
    """
    Quick validation of SSRL response structure.

    Args:
        text: Text to validate

    Returns:
        Dictionary with validation results
    """
    try:
        format_reward = get_format_reward()
        result = format_reward.calculate_reward(text)

        return {
            'is_valid': result.success and result.score > 0.5,
            'score': result.score,
            'missing_tags': [
                tag for tag in DEFAULT_REQUIRED_TAGS
                if tag not in text.lower()
            ],
            'has_structure': '<think>' in text and '<answer>' in text,
            'details': result.details
        }

    except Exception as e:
        return {
            'is_valid': False,
            'score': 0.0,
            'error': str(e),
            'missing_tags': DEFAULT_REQUIRED_TAGS,
            'has_structure': False,
            'details': {}
        }
