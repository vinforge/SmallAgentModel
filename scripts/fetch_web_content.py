#!/usr/bin/env python3
"""
Manual Web Content Fetcher - Phase 7.1 CLI Tool

This script provides a command-line interface for fetching web content
using SAM's isolated web retrieval system. Content is saved to a quarantine
directory for manual review before ingestion into SAM's knowledge base.

Usage:
    python scripts/fetch_web_content.py "https://example.com"
    python scripts/fetch_web_content.py "https://example.com" --timeout 60
    python scripts/fetch_web_content.py "https://example.com" --output-dir custom_quarantine

Features:
- Process isolation for safe web browsing
- Automatic quarantine directory management
- Structured JSON output with metadata
- Comprehensive error handling and logging
- Progress indicators and status reporting

Security:
- All web operations run in isolated subprocesses
- Content is saved to quarantine for manual review
- No automatic ingestion into SAM's knowledge base
- Comprehensive logging for audit trails
"""

import sys
import argparse
import json
from pathlib import Path
from urllib.parse import urlparse
from datetime import datetime
import logging

# Add project root to Python path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

try:
    from web_retrieval import WebFetcher, WebRetrievalError
    from web_retrieval.data_contracts import sanitize_filename, QuarantineMetadata
except ImportError as e:
    print(f"❌ Error importing web retrieval modules: {e}")
    print("Make sure you're running from the SAM project root directory.")
    sys.exit(1)


def setup_logging(level: str = 'INFO') -> logging.Logger:
    """Setup logging configuration."""
    logging.basicConfig(
        level=getattr(logging, level.upper()),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout)
        ]
    )
    return logging.getLogger(__name__)


def create_quarantine_directory(output_dir: str) -> Path:
    """Create and setup quarantine directory."""
    quarantine_path = project_root / output_dir
    quarantine_path.mkdir(exist_ok=True)
    
    # Create README if it doesn't exist
    readme_path = quarantine_path / "README.md"
    if not readme_path.exists():
        readme_content = """# SAM Web Content Quarantine

This directory contains web content fetched by SAM's web retrieval system.
All content here is considered **untrusted** and should be reviewed before
ingestion into SAM's knowledge base.

## File Structure

- `*.json` - Web content data files with metadata
- `metadata.json` - Quarantine directory metadata
- `README.md` - This file

## Security Notice

⚠️ **IMPORTANT**: All files in this directory come from external web sources
and should be treated as potentially unsafe. Review content carefully before
uploading to SAM.

## Usage

1. Review the JSON files for malicious or inappropriate content
2. If content is safe, upload through SAM's document interface
3. Delete or archive processed files to keep quarantine clean

Generated by SAM Web Retrieval System v1.0
"""
        readme_path.write_text(readme_content)
    
    return quarantine_path


def save_quarantine_metadata(quarantine_dir: Path, url: str, filename: str, 
                           content_length: int, success: bool) -> None:
    """Save metadata about quarantined content."""
    metadata_file = quarantine_dir / "metadata.json"
    
    # Load existing metadata
    if metadata_file.exists():
        try:
            with open(metadata_file, 'r') as f:
                metadata = json.load(f)
        except Exception:
            metadata = {"entries": []}
    else:
        metadata = {"entries": []}
    
    # Add new entry
    entry = QuarantineMetadata(
        filename=filename,
        source_url=url,
        fetch_timestamp=datetime.now().isoformat(),
        content_length=content_length,
        fetch_method="manual_cli",
        security_status="untrusted",
        review_status="pending",
        tags=["cli_fetch", "manual", "untrusted"]
    )
    
    metadata["entries"].append(entry.to_dict())
    metadata["last_updated"] = datetime.now().isoformat()
    metadata["total_entries"] = len(metadata["entries"])
    
    # Save updated metadata
    with open(metadata_file, 'w') as f:
        json.dump(metadata, f, indent=2)


def print_success_summary(result, output_path: Path, quarantine_dir: Path):
    """Print success summary with next steps."""
    print("\n" + "="*60)
    print("✅ WEB CONTENT FETCH SUCCESSFUL!")
    print("="*60)
    print(f"📄 Content Length: {len(result.content):,} characters")
    print(f"💾 Saved to: {output_path}")
    print(f"🕒 Timestamp: {result.timestamp}")
    print(f"📁 Quarantine Dir: {quarantine_dir}")
    
    if result.metadata:
        print(f"🔧 Fetch Method: {result.metadata.get('fetch_method', 'unknown')}")
        if result.metadata.get('content_truncated'):
            print("⚠️  Content was truncated due to size limits")
    
    print("\n📋 NEXT STEPS:")
    print("1. 🔍 Review the content in the JSON file for safety")
    print("2. 📤 If safe, upload through SAM's Documents interface")
    print("3. 🗑️  Delete or archive the file after processing")
    print("\n⚠️  SECURITY REMINDER: Content is untrusted until reviewed!")


def print_error_summary(result, output_path: Path):
    """Print error summary with troubleshooting info."""
    print("\n" + "="*60)
    print("❌ WEB CONTENT FETCH FAILED!")
    print("="*60)
    print(f"🌐 URL: {result.url}")
    print(f"❌ Error: {result.error}")
    print(f"💾 Error details saved to: {output_path}")
    
    if result.metadata:
        error_type = result.metadata.get('error_type', 'Unknown')
        print(f"🔧 Error Type: {error_type}")
    
    print("\n🔧 TROUBLESHOOTING:")
    print("- Check if the URL is accessible in your browser")
    print("- Verify your internet connection")
    print("- Try a different URL to test the system")
    print("- Check the error details in the saved JSON file")


def main():
    """Main CLI entry point."""
    parser = argparse.ArgumentParser(
        description='Fetch web content using SAM\'s isolated web retrieval system',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python scripts/fetch_web_content.py "https://example.com"
  python scripts/fetch_web_content.py "https://news.ycombinator.com" --timeout 60
  python scripts/fetch_web_content.py "https://example.com" --output-dir custom_quarantine

Security Note:
  All fetched content is saved to quarantine for manual review.
  No content is automatically added to SAM's knowledge base.
        """
    )
    
    parser.add_argument('url', help='URL to fetch content from')
    parser.add_argument('--timeout', type=int, default=30, 
                       help='Timeout in seconds (default: 30)')
    parser.add_argument('--output-dir', default='quarantine', 
                       help='Output directory (default: quarantine)')
    parser.add_argument('--max-length', type=int, default=1000000,
                       help='Maximum content length (default: 1MB)')
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='Enable verbose logging')
    
    args = parser.parse_args()
    
    # Setup logging
    log_level = 'DEBUG' if args.verbose else 'INFO'
    logger = setup_logging(log_level)
    
    # Validate arguments
    if args.timeout <= 0:
        print("❌ Error: Timeout must be positive")
        sys.exit(1)
    
    if args.max_length <= 0:
        print("❌ Error: Max length must be positive")
        sys.exit(1)
    
    # Create quarantine directory
    try:
        quarantine_dir = create_quarantine_directory(args.output_dir)
        logger.info(f"Quarantine directory ready: {quarantine_dir}")
    except Exception as e:
        print(f"❌ Error creating quarantine directory: {e}")
        sys.exit(1)
    
    # Display startup info
    print("🌐 SAM Web Content Fetcher v1.0")
    print("="*40)
    print(f"📍 Target URL: {args.url}")
    print(f"⏱️  Timeout: {args.timeout} seconds")
    print(f"📁 Output Directory: {quarantine_dir}")
    print(f"📏 Max Content Length: {args.max_length:,} bytes")
    print("\n🚀 Starting isolated web fetch...")
    
    # Initialize fetcher
    try:
        fetcher = WebFetcher(
            timeout=args.timeout,
            max_content_length=args.max_length
        )
        logger.info("WebFetcher initialized successfully")
    except Exception as e:
        print(f"❌ Error initializing WebFetcher: {e}")
        sys.exit(1)
    
    # Fetch content
    try:
        print("🔄 Fetching content (this may take a moment)...")
        result = fetcher.fetch_url_content(args.url)
        
        # Generate filename
        filename = sanitize_filename(args.url)
        output_path = quarantine_dir / filename
        
        # Save result to quarantine
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(result.to_web_content_data().to_dict(), f, indent=2, ensure_ascii=False)
        
        # Save quarantine metadata
        content_length = len(result.content) if result.content else 0
        save_quarantine_metadata(quarantine_dir, args.url, filename, content_length, result.success)
        
        # Display results
        if result.success:
            print_success_summary(result, output_path, quarantine_dir)
            sys.exit(0)
        else:
            print_error_summary(result, output_path)
            sys.exit(1)
            
    except WebRetrievalError as e:
        print(f"❌ Web retrieval error: {e}")
        logger.error(f"Web retrieval error: {e}")
        sys.exit(1)
    except KeyboardInterrupt:
        print("\n⚠️ Operation cancelled by user")
        sys.exit(1)
    except Exception as e:
        print(f"❌ Unexpected error: {e}")
        logger.error(f"Unexpected error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
