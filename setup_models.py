#!/usr/bin/env python3
"""
SAM Model Setup Script
Downloads and initializes required models for SAM Community Edition.
"""

import os
import sys
import subprocess
import time
from pathlib import Path

def print_banner():
    """Print setup banner."""
    print("ğŸ¤– SAM MODEL SETUP")
    print("=" * 80)
    print("Setting up AI models for SAM Community Edition...")
    print("This will download required models for document processing and AI responses.")
    print("=" * 80)

def check_internet_connection():
    """Check if internet connection is available."""
    print("\nğŸŒ Checking internet connection...")
    try:
        import requests
        response = requests.get("https://huggingface.co", timeout=10)
        if response.status_code == 200:
            print("âœ… Internet connection available")
            return True
        else:
            print("âŒ Internet connection issues")
            return False
    except Exception as e:
        print(f"âŒ No internet connection: {e}")
        return False

def setup_sentence_transformers():
    """Setup sentence transformers model for embeddings."""
    print("\nğŸ“š Setting up Sentence Transformers (for document processing)...")
    print("This model enables SAM to understand and search through your documents.")
    
    try:
        # Import and initialize sentence transformers
        from sentence_transformers import SentenceTransformer
        
        print("â¬‡ï¸  Downloading all-MiniLM-L6-v2 model (first time only)...")
        print("   Size: ~90MB - This may take a few minutes...")
        
        # Create models directory
        models_dir = Path("models/embeddings")
        models_dir.mkdir(parents=True, exist_ok=True)
        
        # Download and cache the model
        model = SentenceTransformer('all-MiniLM-L6-v2', cache_folder=str(models_dir))
        
        # Test the model
        test_embedding = model.encode("This is a test sentence.")
        print(f"âœ… Sentence Transformers model ready (dimension: {len(test_embedding)})")
        
        return True
        
    except ImportError:
        print("âŒ sentence-transformers not installed")
        print("ğŸ”§ Installing sentence-transformers...")
        try:
            subprocess.run([sys.executable, "-m", "pip", "install", "sentence-transformers"], 
                         check=True, capture_output=True)
            print("âœ… sentence-transformers installed")
            return setup_sentence_transformers()  # Retry
        except Exception as e:
            print(f"âŒ Failed to install sentence-transformers: {e}")
            return False
    except Exception as e:
        print(f"âŒ Error setting up sentence transformers: {e}")
        return False

def check_ollama_installation():
    """Check if Ollama is installed and running."""
    print("\nğŸ¦™ Checking Ollama installation...")
    
    try:
        # Check if ollama command exists
        result = subprocess.run(["ollama", "--version"], 
                              capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            print("âœ… Ollama is installed")
            
            # Check if Ollama is running
            try:
                import requests
                response = requests.get("http://localhost:11434/api/tags", timeout=5)
                if response.status_code == 200:
                    print("âœ… Ollama is running")
                    return True
                else:
                    print("âš ï¸  Ollama installed but not running")
                    print("ğŸ”§ Please start Ollama:")
                    print("   â€¢ Windows/Mac: Start Ollama app")
                    print("   â€¢ Linux: ollama serve")
                    return False
            except Exception:
                print("âš ï¸  Ollama installed but not running")
                print("ğŸ”§ Please start Ollama and run this script again")
                return False
        else:
            print("âŒ Ollama not found")
            return False
            
    except FileNotFoundError:
        print("âŒ Ollama not installed")
        return False
    except Exception as e:
        print(f"âŒ Error checking Ollama: {e}")
        return False

def setup_ollama_model():
    """Setup the main LLM model via Ollama."""
    print("\nğŸ§  Setting up Main AI Model (via Ollama)...")
    
    if not check_ollama_installation():
        print("\nğŸ“‹ OLLAMA INSTALLATION REQUIRED:")
        print("SAM uses Ollama for AI responses. Please install it:")
        print("â€¢ Visit: https://ollama.ai")
        print("â€¢ Download and install Ollama for your platform")
        print("â€¢ Start Ollama, then run this script again")
        return False
    
    try:
        # Check what models are available
        import requests
        response = requests.get("http://localhost:11434/api/tags", timeout=10)
        
        if response.status_code == 200:
            models = response.json().get('models', [])
            model_names = [model['name'] for model in models]
            
            # Check if we have SAM's preferred model or suitable alternatives
            preferred_models = [
                "hf.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF:Q4_K_M",  # SAM's default
                "qwen2.5:7b",
                "qwen2.5:3b",
                "llama3.2:3b",
                "llama3.2:1b",
                "phi3:mini"
            ]
            
            available_model = None
            for model in preferred_models:
                if any(model in name for name in model_names):
                    available_model = model
                    break
            
            if available_model:
                print(f"âœ… Found suitable model: {available_model}")
                return True
            else:
                print("âš ï¸  No suitable model found")
                print("ğŸ”§ Downloading SAM's recommended model...")

                # Download SAM's default model (DeepSeek-R1 Qwen 8B Q4)
                model_to_download = "hf.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF:Q4_K_M"
                print(f"â¬‡ï¸  Downloading {model_to_download}")
                print("   This is SAM's default model (DeepSeek-R1 Qwen 8B, 4-bit quantized)")
                print("   Size: ~4.3GB - This may take 10-20 minutes depending on connection...")
                
                result = subprocess.run(
                    ["ollama", "pull", model_to_download],
                    capture_output=True, text=True, timeout=1800  # 30 minutes
                )
                
                if result.returncode == 0:
                    print(f"âœ… SAM's default model downloaded successfully")
                    print("ğŸ‰ You now have SAM's full AI capabilities!")
                    return True
                else:
                    print(f"âŒ Failed to download SAM's default model: {result.stderr}")
                    print("ğŸ”§ Trying fallback to smaller model...")

                    # Fallback to smaller model
                    fallback_model = "qwen2.5:3b"
                    print(f"â¬‡ï¸  Downloading fallback model: {fallback_model}")

                    fallback_result = subprocess.run(
                        ["ollama", "pull", fallback_model],
                        capture_output=True, text=True, timeout=900  # 15 minutes
                    )

                    if fallback_result.returncode == 0:
                        print(f"âœ… Fallback model {fallback_model} downloaded successfully")
                        return True
                    else:
                        print("âŒ Fallback model download also failed")
                        print("ğŸ”§ You can manually download a model later:")
                        print("   ollama pull qwen2.5:3b")
                        print("   ollama pull hf.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF:Q4_K_M")
                        return False
        else:
            print("âŒ Could not connect to Ollama")
            return False
            
    except Exception as e:
        print(f"âŒ Error setting up Ollama model: {e}")
        return False

def create_model_config():
    """Create model configuration file."""
    print("\nâš™ï¸  Creating model configuration...")
    
    config_content = """# SAM Model Configuration
# This file is created automatically by setup_models.py

[embedding_model]
name = "all-MiniLM-L6-v2"
provider = "sentence-transformers"
cache_dir = "models/embeddings"

[llm_model]
provider = "ollama"
api_url = "http://localhost:11434"
default_model = "hf.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF:Q4_K_M"
fallback_models = ["qwen2.5:7b", "qwen2.5:3b", "llama3.2:3b", "phi3:mini"]

[model_settings]
max_context_length = 4096
temperature = 0.7
max_tokens = 1000
timeout_seconds = 60
"""
    
    try:
        config_dir = Path("config")
        config_dir.mkdir(exist_ok=True)
        
        config_file = config_dir / "models.conf"
        with open(config_file, "w") as f:
            f.write(config_content)
        
        print(f"âœ… Model configuration saved to {config_file}")
        return True
        
    except Exception as e:
        print(f"âŒ Error creating model config: {e}")
        return False

def show_setup_summary(embedding_success, ollama_success, config_success):
    """Show setup summary."""
    print(f"\nğŸ“Š MODEL SETUP SUMMARY")
    print("=" * 80)
    
    print(f"ğŸ“š Embedding Model (Document Processing): {'âœ… SUCCESS' if embedding_success else 'âŒ FAILED'}")
    print(f"ğŸ§  LLM Model (AI Responses): {'âœ… SUCCESS' if ollama_success else 'âŒ FAILED'}")
    print(f"âš™ï¸  Configuration: {'âœ… SUCCESS' if config_success else 'âŒ FAILED'}")
    
    total_success = sum([embedding_success, ollama_success, config_success])
    
    print(f"\nğŸ“Š Overall: {total_success}/3 components ready")
    
    if total_success == 3:
        print(f"\nğŸ‰ ALL MODELS READY!")
        print(f"âœ… SAM is fully configured for AI functionality")
        print(f"âœ… Document processing enabled")
        print(f"âœ… AI responses enabled")
        print(f"ğŸš€ You can now start SAM with: python start_sam.py")
        
    elif total_success >= 2:
        print(f"\nâœ… MOSTLY READY!")
        print(f"âœ… Core functionality available")
        if not ollama_success:
            print(f"âš ï¸  Install Ollama for full AI responses")
        
    else:
        print(f"\nâŒ SETUP INCOMPLETE")
        print(f"âŒ Some models failed to install")
        print(f"ğŸ”§ Check error messages above and try again")
    
    print(f"\nğŸ’¡ WHAT THESE MODELS DO:")
    print(f"ğŸ“š Embedding Model (all-MiniLM-L6-v2): Document search and understanding")
    print(f"ğŸ§  LLM Model (DeepSeek-R1 Qwen 8B Q4): Advanced AI responses and reasoning")
    print(f"âš™ï¸  Configuration: Optimizes performance for your system")

if __name__ == "__main__":
    print_banner()
    
    # Check prerequisites
    if not check_internet_connection():
        print("\nâŒ Internet connection required for model downloads")
        print("ğŸ”§ Please check your connection and try again")
        sys.exit(1)
    
    # Setup models
    embedding_success = setup_sentence_transformers()
    ollama_success = setup_ollama_model()
    config_success = create_model_config()
    
    # Show summary
    show_setup_summary(embedding_success, ollama_success, config_success)
    
    print(f"\nğŸ MODEL SETUP COMPLETE!")
    
    if embedding_success and ollama_success:
        print("ğŸ‰ SAM is ready with full AI capabilities!")
    elif embedding_success:
        print("ğŸ“š SAM ready for document processing (install Ollama for AI chat)")
    else:
        print("ğŸ”§ Please resolve issues above and run again")
    
    print(f"\nğŸ“‹ NEXT STEPS:")
    print(f"1. âœ… Models configured")
    print(f"2. ğŸ” Run: python setup_encryption.py (if not done)")
    print(f"3. ğŸš€ Run: python start_sam.py")
    
    print(f"\nğŸŒŸ Welcome to SAM Community Edition!")
